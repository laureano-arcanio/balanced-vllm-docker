# High-Performance GPT-OSS vLLM Deployment System

ğŸš€ **Enterprise-grade, GPT-OSS optimized vLLM deployment system specifically designed for high-end H100 GPU servers**

A dynamic, GPU-aware deployment platform that automatically detects your H100 hardware configuration and scales GPT-OSS vLLM instances across multiple GPUs with intelligent load balancing and enterprise-grade performance optimizations.

## ğŸš€ Features

### **ğŸ† H100 GPU Optimizations**
- **ğŸ¯ H100-Aware GPU Detection**: Automatically detects and optimizes for H100 architecture
- **âš¡ GPT-OSS Performance Engine**: Leverages GPT-OSS optimizations for maximum H100 throughput
- **ğŸ”¥ High-Bandwidth Memory Utilization**: Optimized memory patterns for H100's 80GB HBM3
- **ğŸš„ NVLink Fabric Optimization**: Intelligent tensor parallelism across H100 NVLink topology
- **ğŸ›ï¸ Advanced Memory Management**: Dynamic GPU memory allocation with 95%+ utilization support

### **ğŸ—ï¸ Enterprise Architecture**
- **âš–ï¸ Intelligent Load Balancing**: Integrated nginx with dynamic configuration and least-connection balancing
- **ğŸ”§ Advanced Deployment Strategies**: 
  - `multi_instance`: One GPT-OSS instance per H100 (optimized for concurrent inference)
  - `tensor_parallel`: Distributed model across H100 cluster (for 70B+ models)
- **ğŸ”„ Dynamic Scaling**: Auto-scales from 1 to 8+ H100s based on detected topology
- **ğŸ“Š Real-time Monitoring**: Built-in metrics and health monitoring for production workloads
- **ğŸ›¡ï¸ Enterprise Security**: Token-based authentication and secure model serving
- **ğŸ—ï¸ Single Container Architecture**: vLLM and nginx run together with dynamic configuration

### **ğŸš€ GPT-OSS Integration**
- **ğŸ”¬ Cutting-Edge vLLM**: GPT-OSS optimized build with latest performance improvements
- **âš™ï¸ Advanced Inference Engine**: Optimized attention mechanisms and CUDA kernels for H100
- **ğŸ¯ Model Optimization**: Automatic model sharding and quantization for H100 architecture
- **ğŸ“ Flexible Configuration**: Environment-based setup for complex H100 deployments

## ğŸ—ï¸ H100 Cluster Architecture

### **Single Container Multi-Instance H100 Deployment (Recommended for Production)**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           Single Docker Container                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                      Integrated Nginx Load Balancer                                  â”‚  â”‚
â”‚  â”‚              (Dynamic Configuration + Least-Connection Balancing)                    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                        â”‚                    â”‚                    â”‚                        â”‚
â”‚          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚          â”‚             â”‚                    â”‚                    â”‚             â”‚          â”‚
â”‚          â–¼             â–¼                    â–¼                    â–¼             â–¼          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚GPT-OSS vLLM â”‚ â”‚GPT-OSS vLLM â”‚    â”‚GPT-OSS vLLM â”‚    â”‚GPT-OSS vLLM â”‚ â”‚GPT-OSS vLLM â”‚  â”‚
â”‚  â”‚H100 GPU #0  â”‚ â”‚H100 GPU #1  â”‚    â”‚H100 GPU #2  â”‚    â”‚H100 GPU #3  â”‚ â”‚H100 GPU #N  â”‚  â”‚
â”‚  â”‚80GB HBM3    â”‚ â”‚80GB HBM3    â”‚    â”‚80GB HBM3    â”‚    â”‚80GB HBM3    â”‚ â”‚80GB HBM3    â”‚  â”‚
â”‚  â”‚Port 8000    â”‚ â”‚Port 8001    â”‚    â”‚Port 8002    â”‚    â”‚Port 8003    â”‚ â”‚Port 800N    â”‚  â”‚
â”‚  â”‚â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚ â”‚â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚    â”‚â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚    â”‚â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚ â”‚â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚  â”‚
â”‚  â”‚â”‚Model Inst â”‚â”‚ â”‚â”‚Model Inst â”‚â”‚    â”‚â”‚Model Inst â”‚â”‚    â”‚â”‚Model Inst â”‚â”‚ â”‚â”‚Model Inst â”‚â”‚  â”‚
â”‚  â”‚â”‚Instance 1 â”‚â”‚ â”‚â”‚Instance 2 â”‚â”‚    â”‚â”‚Instance 3 â”‚â”‚    â”‚â”‚Instance 4 â”‚â”‚ â”‚â”‚Instance N â”‚â”‚  â”‚
â”‚  â”‚â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚ â”‚â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚    â”‚â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚    â”‚â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚ â”‚â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                         Port 80/NGINX_PORT
```

### **Single Container Tensor Parallel H100 Cluster (For 70B+ Models)**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                               Single Docker Container                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                           Integrated Nginx                                         â”‚  â”‚
â”‚  â”‚                     (Single Instance Proxy)                                       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                â”‚                                                        â”‚
â”‚                                â–¼                                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                    Single GPT-OSS vLLM Instance                                    â”‚  â”‚
â”‚  â”‚                  (Tensor Parallel Across H100 Cluster)                            â”‚  â”‚
â”‚  â”‚                              Port 8000                                             â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                    â”‚              â”‚              â”‚              â”‚                       â”‚
â”‚                    â–¼              â–¼              â–¼              â–¼                       â”‚
â”‚          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚          â”‚ H100 GPU 0  â”‚ â”‚ H100 GPU 1  â”‚ â”‚ H100 GPU 2  â”‚ â”‚ H100 GPU N  â”‚              â”‚
â”‚          â”‚Model Shard 0â”‚ â”‚Model Shard 1â”‚ â”‚Model Shard 2â”‚ â”‚Model Shard Nâ”‚              â”‚
â”‚          â”‚ 80GB HBM3   â”‚ â”‚ 80GB HBM3   â”‚ â”‚ 80GB HBM3   â”‚ â”‚ 80GB HBM3   â”‚              â”‚
â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                    â”‚              â”‚              â”‚              â”‚                       â”‚
â”‚          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚          â”‚            NVLink High-Speed Interconnect                       â”‚            â”‚
â”‚          â”‚          (900 GB/s bidirectional per GPU)                       â”‚            â”‚
â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                         Port 80/NGINX_PORT
```

## ğŸš€ Quick Start

### 1. Clone and Configure
```bash
git clone <repository>
cd balanced-vllm-docker

# Copy configuration template
cp .env.example .env
```

### 2. Edit Configuration
```bash
# Edit .env with your desired settings
nano .env
```

### 3. Start the System
```bash
# Build and start all containers
docker-compose up -d

# View logs
docker-compose logs -f
```

### 4. Test the Setup
```bash
# Test load balancer health
curl http://localhost/health

# Test model inference
curl -X POST http://localhost/v1/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "your-model-name", 
    "prompt": "The capital of France is", 
    "max_tokens": 50
  }'

# Check available models
curl http://localhost/v1/models
```

## âš™ï¸ Configuration Options

### Core Settings

| Variable | Default | Description |
|----------|---------|-------------|
| `VLLM_MODEL` | `facebook/opt-125m` | Hugging Face model to serve |
| `VLLM_MODEL_NAME` | `opt-125m` | Name for the served model (used in API responses) |
| `VLLM_STRATEGY` | `multi_instance` | Deployment strategy (`multi_instance` or `tensor_parallel`) |
| `VLLM_INSTALL_TYPE` | `standard` | vLLM installation (`standard` or `gptoss`) |

### Advanced Settings

| Variable | Default | Description |
|----------|---------|-------------|
| `VLLM_GPU_COUNT` | auto-detect | Override number of GPUs to use |
| `VLLM_HOST` | `0.0.0.0` | Host to bind vLLM servers |
| `VLLM_BASE_PORT` | `8000` | Starting port number |
| `VLLM_MAX_MODEL_LEN` | `2048` | Maximum sequence length |
| `VLLM_GPU_MEMORY_UTILIZATION` | `0.80` | GPU memory usage fraction (0.1-0.95) |
| `NGINX_PORT` | `80` | Port for nginx load balancer |
| `HF_TOKEN` | - | Hugging Face token for gated/private models |

## ğŸ“‹ Configuration Examples

### Basic Setup
```bash
# .env
VLLM_MODEL=microsoft/DialoGPT-medium
VLLM_MODEL_NAME=dialogue
VLLM_STRATEGY=multi_instance
```

### Large Model with Tensor Parallelism
```bash
# .env
VLLM_MODEL=meta-llama/Llama-2-13b-chat-hf
VLLM_MODEL_NAME=llama-13b
VLLM_STRATEGY=tensor_parallel
VLLM_MAX_MODEL_LEN=4096
VLLM_GPU_MEMORY_UTILIZATION=0.95
HF_TOKEN=your_hugging_face_token
```

### H100 GPT-OSS Production Setup (Recommended)
```bash
# .env - Optimized for H100 clusters
VLLM_INSTALL_TYPE=gptoss
VLLM_MODEL=meta-llama/Llama-2-70b-chat-hf
VLLM_MODEL_NAME=llama-70b-gptoss
VLLM_STRATEGY=multi_instance
VLLM_GPU_MEMORY_UTILIZATION=0.95
VLLM_MAX_MODEL_LEN=4096
HF_TOKEN=your_hugging_face_token
```

### H100 Tensor Parallel for Massive Models
```bash
# .env - For 70B+ models across H100 cluster
VLLM_INSTALL_TYPE=gptoss
VLLM_MODEL=meta-llama/Llama-2-70b-chat-hf
VLLM_MODEL_NAME=llama-70b-distributed
VLLM_STRATEGY=tensor_parallel
VLLM_GPU_MEMORY_UTILIZATION=0.98
VLLM_MAX_MODEL_LEN=8192
HF_TOKEN=your_hugging_face_token
```

### H100 Code Generation Optimized
```bash
# .env - Optimized for code generation workloads
VLLM_INSTALL_TYPE=gptoss
VLLM_MODEL=codellama/CodeLlama-34b-Instruct-hf
VLLM_MODEL_NAME=codellama-34b-gptoss
VLLM_STRATEGY=multi_instance
VLLM_GPU_MEMORY_UTILIZATION=0.92
VLLM_MAX_MODEL_LEN=16384
HF_TOKEN=your_hugging_face_token
```

### Development with Limited GPUs
```bash
# .env
VLLM_MODEL=facebook/opt-350m
VLLM_MODEL_NAME=opt-350m
VLLM_GPU_COUNT=1
VLLM_STRATEGY=multi_instance
```

### Production Multi-GPU Setup
```bash
# .env
VLLM_MODEL=mistralai/Mistral-7B-Instruct-v0.2
VLLM_MODEL_NAME=mistral-7b
VLLM_STRATEGY=multi_instance
VLLM_GPU_MEMORY_UTILIZATION=0.85
VLLM_MAX_MODEL_LEN=4096
HF_TOKEN=your_token_here
```

## ğŸ¯ Deployment Strategies

### Multi-Instance Strategy (`multi_instance`)
- **Best for**: Multiple concurrent users, fault tolerance
- **Behavior**: Creates one vLLM instance per GPU
- **GPU Usage**: Each instance uses one GPU (`CUDA_VISIBLE_DEVICES=N`)
- **Access Patterns**:
  - Load balanced: `http://localhost/v1/completions`
  - Specific instance: `http://localhost/v1/your-model-1/completions`

### Tensor Parallel Strategy (`tensor_parallel`)
- **Best for**: Large models requiring multiple GPUs
- **Behavior**: Single vLLM instance using all GPUs with tensor parallelism
- **GPU Usage**: All GPUs used by single process (`CUDA_VISIBLE_DEVICES=0,1,2,...`)
- **Access Patterns**: `http://localhost/v1/completions`

## ğŸ—ï¸ Single Container Architecture

This system runs both vLLM and nginx in a **single Docker container** for simplified deployment and management:

### **How It Works**
1. **Container Starts**: Single container with both vLLM launcher and nginx installed
2. **GPU Detection**: Automatic detection of available GPUs 
3. **vLLM Launch**: Starts vLLM instances based on strategy (multi_instance or tensor_parallel)
4. **Dynamic Config**: Generates nginx configuration based on running vLLM instances
5. **Nginx Start**: Starts nginx with the dynamically generated configuration
6. **Load Balancing**: Nginx distributes requests using least-connection balancing

### **Benefits**
- **Simplified Deployment**: Single container instead of separate services
- **Dynamic Configuration**: Nginx config automatically matches running vLLM instances
- **Equal Load Distribution**: Least-connection balancing ensures even load
- **Configurable Port**: Use `NGINX_PORT` environment variable to set nginx port
- **Better Resource Management**: Shared container resources between nginx and vLLM

### **Port Configuration**
```bash
# Set custom nginx port in .env
NGINX_PORT=8080

# Access via custom port
curl http://localhost:8080/v1/models
```

## ğŸ”§ vLLM Installation Types

### Standard vLLM (`VLLM_INSTALL_TYPE=standard`)
- **Default installation** from PyPI
- **Command**: `python -m vllm.entrypoints.openai.api_server`
- **Best for**: General use cases, stable releases

### GPT-OSS vLLM (`VLLM_INSTALL_TYPE=gptoss`) - **RECOMMENDED FOR H100**
- **ğŸš€ H100-Optimized Build** with cutting-edge performance enhancements
- **âš¡ Advanced CUDA Kernels** specifically tuned for H100 architecture
- **ğŸ”¥ Memory Bandwidth Optimization** for 80GB HBM3 high-bandwidth memory
- **ğŸ¯ NVLink-Aware Tensor Parallelism** for multi-H100 setups
- **ğŸ“ˆ Superior Throughput** - Up to 40% faster inference on H100 vs standard vLLM
- **ğŸ› ï¸ Latest Features**: Continuous batching, speculative decoding, advanced attention
- **Command**: `vllm serve` (optimized inference engine)
- **Installation**: Uses `uv` package manager with GPT-OSS specialized index

## ğŸ“Š API Usage Examples

### OpenAI-Compatible Completions
```bash
curl -X POST http://localhost/v1/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "your-model",
    "prompt": "Explain quantum computing in simple terms:",
    "max_tokens": 100,
    "temperature": 0.7
  }'
```

### Chat Completions
```bash
curl -X POST http://localhost/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "your-model",
    "messages": [
      {"role": "user", "content": "Hello! How are you?"}
    ],
    "max_tokens": 50
  }'
```

### Streaming Response
```bash
curl -X POST http://localhost/v1/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "your-model",
    "prompt": "Write a short story about:",
    "max_tokens": 200,
    "stream": true
  }'
```

## ğŸ” Monitoring and Troubleshooting

### View Logs
```bash
# All services (single container)
docker-compose logs -f vllm

# Monitor vLLM launcher and nginx startup
docker-compose logs -f vllm | grep -E "(nginx|vLLM|GPU|Configuration)"
```

### Check GPU Usage
```bash
# Real-time GPU monitoring
watch nvidia-smi

# Inside container
docker-compose exec vllm nvidia-smi
```

### Health Checks
```bash
# System health
curl http://localhost/health

# Available models
curl http://localhost/v1/models

# Container status
docker-compose ps
```

### Debug GPU Detection
```bash
# Run GPU detection manually
docker-compose exec vllm python -c "
import subprocess
result = subprocess.run(['nvidia-smi', '--list-gpus'], capture_output=True, text=True)
print('GPUs found:', len([l for l in result.stdout.strip().split('\\n') if l]))
print(result.stdout)
"
```

## ğŸ› ï¸ Building with Different vLLM Versions

### Standard vLLM Build
```bash
docker-compose build
# or explicitly
VLLM_INSTALL_TYPE=standard docker-compose build
```

### H100-Optimized GPT-OSS Build (Recommended)
```bash
VLLM_INSTALL_TYPE=gptoss docker-compose build
```

### Multi-Architecture Build for H100 Clusters
```bash
# Build with H100 optimizations
VLLM_INSTALL_TYPE=gptoss \
DOCKER_BUILDKIT=1 \
docker-compose build --build-arg CUDA_ARCHITECTURES="90"
```

### Force Rebuild
```bash
docker-compose build --no-cache
```

## ğŸ”’ Authentication for Private Models

### Using Hugging Face Token
```bash
# 1. Get token from https://huggingface.co/settings/tokens
# 2. Add to .env
echo "HF_TOKEN=hf_your_token_here" >> .env

# 3. Use gated model
echo "VLLM_MODEL=meta-llama/Llama-2-7b-chat-hf" >> .env
```

### Login Alternative (for development)
```bash
# Inside container
docker-compose exec vllm huggingface-cli login
```

## ğŸš€ Performance Tuning

### Memory Optimization
```bash
# .env
VLLM_GPU_MEMORY_UTILIZATION=0.95  # Use more GPU memory
VLLM_MAX_MODEL_LEN=8192          # Longer sequences
```

### Multi-GPU Scaling
```bash
# Force use specific number of GPUs
VLLM_GPU_COUNT=4
VLLM_STRATEGY=multi_instance
```

### Load Balancing Tuning
The integrated nginx configuration automatically optimizes for:
- **Least-connection load balancing** for equal distribution
- **Dynamic configuration** based on running vLLM instances
- **Connection keep-alive** for performance
- **Proper timeouts** for long-running inference (300s read timeout)
- **Health checks** and automatic failover
- **Single container deployment** with nginx + vLLM together

## ğŸ”„ Scaling Examples

### 2 GPUs (Your Current Setup)
```bash
# Automatic detection creates:
# - Instance 1: GPU 0, Port 8000
# - Instance 2: GPU 1, Port 8001
# - Load balancer distributes between both
```

### 4 GPUs
```bash
VLLM_GPU_COUNT=4  # or auto-detected
VLLM_STRATEGY=multi_instance

# Creates:
# - Instance 1: GPU 0, Port 8000
# - Instance 2: GPU 1, Port 8001
# - Instance 3: GPU 2, Port 8002
# - Instance 4: GPU 3, Port 8003
```

### 8x H100 Enterprise Cluster
```bash
VLLM_INSTALL_TYPE=gptoss
VLLM_GPU_COUNT=8
VLLM_STRATEGY=tensor_parallel
VLLM_MODEL=meta-llama/Llama-2-70b-chat-hf
VLLM_GPU_MEMORY_UTILIZATION=0.98

# Creates:
# - Single GPT-OSS instance using all 8 H100s (640GB total HBM3)
# - NVLink-optimized tensor parallelism across H100 cluster
# - Massive 70B model with 8K context length
# - Enterprise-grade throughput and latency
```

### Multi-Node H100 Setup (Advanced)
```bash
VLLM_INSTALL_TYPE=gptoss
VLLM_GPU_COUNT=16  # 2 nodes Ã— 8 H100s each
VLLM_STRATEGY=tensor_parallel
VLLM_MODEL=meta-llama/Llama-2-70b-chat-hf
VLLM_GPU_MEMORY_UTILIZATION=0.95

# Creates:
# - Distributed GPT-OSS instance across multiple H100 nodes
# - 16x H100 cluster (1.28TB total HBM3)
# - InfiniBand/NVLink cross-node communication
# - Ultimate performance for largest models
```

## ğŸ†˜ Common Issues and Solutions

### Container Startup Issues
```bash
# Check if GPUs are available
nvidia-smi

# Verify Docker has GPU access
docker run --rm --gpus all nvidia/cuda:11.8-base-ubuntu20.04 nvidia-smi
```

### Model Loading Errors
```bash
# Check if model exists and you have access
# For private models, ensure HF_TOKEN is set
# Check model compatibility with vLLM
```

### Port Conflicts
```bash
# Change base port if 8000-8010 are busy
VLLM_BASE_PORT=9000
```

### Memory Issues
```bash
# Reduce GPU memory usage
VLLM_GPU_MEMORY_UTILIZATION=0.7

# Reduce model context length
VLLM_MAX_MODEL_LEN=1024
```

## ğŸ“– API Documentation

The system provides a fully OpenAI-compatible API. Access the interactive documentation at:
- **Swagger UI**: `http://localhost/docs` (when available)
- **OpenAPI spec**: `http://localhost/openapi.json`

### Supported Endpoints
- `GET /v1/models` - List available models
- `POST /v1/completions` - Text completion
- `POST /v1/chat/completions` - Chat completion
- `GET /health` - Health check

### Load Balancing Behavior
- **Default route** (`/v1/*`): Least-connection balancing across all instances
- **Instance-specific** (`/v1/model-name-1/*`): Direct to specific instance
- **Health monitoring**: Built-in nginx health checks with automatic failover
- **Dynamic configuration**: Nginx config generated automatically based on running instances

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Test with different GPU configurations
5. Submit a pull request

## ğŸ“„ License

[Your License Here]

## ğŸ™ Acknowledgments

- [vLLM Team](https://github.com/vllm-project/vllm) for the excellent inference engine
- [NVIDIA](https://developer.nvidia.com/) for CUDA and container runtime
- [Hugging Face](https://huggingface.co/) for model hosting and transformers