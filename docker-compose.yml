version: '3.8'

services:
  # Dynamic vLLM Service
  vllm:
    build:
      context: .
      args:
        VLLM_INSTALL_TYPE: ${VLLM_INSTALL_TYPE:-standard}
    container_name: vllm_dynamic
    command: python /app/scripts/vllm_launcher.py
    ports:
      # Expose ports 8000-8010 for multiple instances and nginx port
      - "8000-8010:8000-8010"
      - "${NGINX_PORT:-80}:${NGINX_PORT:-80}"
    volumes:
      - ${MODEL_CACHE_PATH:-./models}:/root/.cache/huggingface  # Cache models locally
    environment:
      - HF_TOKEN=${HF_TOKEN}
      - VLLM_MODEL=${VLLM_MODEL:-facebook/opt-125m}
      - VLLM_MODEL_NAME=${VLLM_MODEL_NAME:-opt-125m}
      - VLLM_STRATEGY=${VLLM_STRATEGY:-multi_instance}
      - VLLM_GPU_COUNT=${VLLM_GPU_COUNT:-2}
      - VLLM_HOST=${VLLM_HOST:-0.0.0.0}
      - VLLM_BASE_PORT=${VLLM_BASE_PORT:-8000}
      - VLLM_MAX_MODEL_LEN=${VLLM_MAX_MODEL_LEN:-2048}
      - VLLM_GPU_MEMORY_UTILIZATION=${VLLM_GPU_MEMORY_UTILIZATION:-0.80}
      - VLLM_INSTALL_TYPE=${VLLM_INSTALL_TYPE:-standard}
      - MODEL_CACHE_PATH=${MODEL_CACHE_PATH:-./models}
      - NGINX_PORT=${NGINX_PORT:-80}
      - TOKENIZERS_PARALLELISM=false
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 15s
      retries: 5
      start_period: 180s


volumes:
  models:
  nginx-config: