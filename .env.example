# Hugging Face token for accessing gated/private models
# Get your token from: https://huggingface.co/settings/tokens
HF_TOKEN=your_hugging_face_token_here

# vLLM Configuration
# Model to serve
VLLM_MODEL=facebook/opt-125m
VLLM_MODEL_NAME=opt-125m

# Deployment strategy: 'multi_instance' or 'tensor_parallel'
# multi_instance: One instance per GPU (better for serving multiple users)
# tensor_parallel: Single instance using all GPUs (better for large models)
VLLM_STRATEGY=multi_instance

# Optional: Override number of instances/GPUs to use (default: auto-detect all)
# VLLM_GPU_COUNT=2

# vLLM server configuration
VLLM_HOST=0.0.0.0
VLLM_BASE_PORT=8000
VLLM_MAX_MODEL_LEN=2048

# GPU memory utilization (0.1 to 0.95)
VLLM_GPU_MEMORY_UTILIZATION=0.80

# vLLM Installation type: 'standard' or 'gptoss'
# standard: Regular vLLM installation
# gptoss: GPT-OSS optimized vLLM with enhanced performance
VLLM_INSTALL_TYPE=standard